{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  50000  100000  150000  200000  250000  300000  350000  400000  450000  500000  550000  600000  total clean lines: 624181\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "def cleaning1(name:str,saved_name:str,encoding:str,extra_stopwords:list=[]):\n",
    "    filer = open(name,\"r\",encoding=encoding)\n",
    "    a=True\n",
    "    count=0\n",
    "    filew = open(saved_name+\".txt\",\"w\",encoding=encoding)\n",
    "    while a:\n",
    "        linea=filer.readline()\n",
    "        if linea==\"\":\n",
    "            a=False\n",
    "        else:\n",
    "            if count%50000==0:\n",
    "                print(count,end=\"  \")\n",
    "            l=linea.split(\"----\")\n",
    "            author_id=l[0]\n",
    "            words=l[1]\n",
    "            for stop_word in extra_stopwords:\n",
    "                words=words.replace(\",\"+stop_word+\",\",\",\")\n",
    "            clean_line = gensim.parsing.preprocessing.remove_stopwords(words.replace(\",\",\" \"))\n",
    "            clean_line=clean_line.replace(\" \",\",\")\n",
    "            clean_line=author_id+\"----\"+clean_line+\"\\n\"\n",
    "            filew.write(clean_line)\n",
    "            count+=1\n",
    "    filer.close()\n",
    "    filew.close()\n",
    "    print(\"total clean lines:\",count)\n",
    "extra_stopwords=[\"s\",\"one\",\"paper\",\"papers\",\"we\",\"present\",\"well\",\"whithin\",\"based\",\"form\",\"structure\",\"show\",\"given\",\"based\",\"results\",\"use\"]\n",
    "cleaning1(\"abstracts_processed.txt\",\"abstracts_processed_cleaned\",\"utf8\",extra_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nicol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 ['arabic', 'azerbaijani', 'bengali', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n",
      "len(stopwords_in_24_languages)= 7113\n",
      "<class 'set'>\n",
      "0  50000  100000  150000  200000  250000  300000  350000  400000  450000  500000  550000  600000  total clean lines: 624181\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "#esto asume que las stopwords de un idioma son stopwords en otro\n",
    "def cleaning2(name:str,saved_name:str,encoding:str,extra_stopwords:set):\n",
    "    filer = open(name,\"r\",encoding=encoding)\n",
    "    a=True\n",
    "    count=0\n",
    "    filew = open(saved_name+\".txt\",\"w\",encoding=encoding)\n",
    "    while a:\n",
    "        linea=filer.readline()\n",
    "        if linea==\"\":\n",
    "            a=False\n",
    "        else:\n",
    "            if count%50000==0:\n",
    "                print(count,end=\"  \")\n",
    "            l=linea.split(\"----\")\n",
    "            author_id=l[0]\n",
    "            words=l[1]\n",
    "            words=\",\".join(word for word in words.split(\",\") if word not in extra_stopwords)\n",
    "            clean_line = gensim.parsing.preprocessing.remove_stopwords(words.replace(\",\",\" \"))\n",
    "            clean_line=clean_line.replace(\" \",\",\")\n",
    "            clean_line=author_id+\"----\"+clean_line+\"\\n\"\n",
    "            filew.write(clean_line)\n",
    "            count+=1\n",
    "    filer.close()\n",
    "    filew.close()\n",
    "    print(\"total clean lines:\",count)\n",
    "\n",
    "\n",
    "print(len(stopwords.fileids()),stopwords.fileids())\n",
    "english_stop_words = set(stopwords.words('english'))\n",
    "french_stop_words = set(stopwords.words(\"french\"))\n",
    "#print(len(english_stop_words))\n",
    "#print(len(french_stop_words))\n",
    "\n",
    "stopwords_in_24_languages=set()\n",
    "for language in stopwords.fileids():\n",
    "    stopwords_in_24_languages |= set(stopwords.words(language))\n",
    "print(\"len(stopwords_in_24_languages)=\",len(stopwords_in_24_languages))\n",
    "\n",
    "extra_stopwords=[\"s\",\"one\",\"paper\",\"papers\",\"we\",\"present\",\"well\",\"whithin\",\"based\",\"form\",\"structure\",\"show\",\"given\",\"based\",\"results\",\"use\"]\n",
    "print(type(set(extra_stopwords)))\n",
    "stopwords_in_24_languages|=set(extra_stopwords)\n",
    "#print(stopwords_in_24_languages)\n",
    "cleaning2(\"abstracts_processed.txt\",\"abstracts_24_languages\",\"utf8\",stopwords_in_24_languages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arabic', 'azerbaijani', 'bengali', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n",
      "24\n",
      "179\n",
      "157\n",
      "7113\n",
      "The time of execution denis : 0.0\n",
      "The time of execution nico : 0.005949735641479492\n",
      "nico gana False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nicol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#testeo de metodos \n",
    "import gensim\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "import time\n",
    "\n",
    "print(stopwords.fileids())\n",
    "print(len(stopwords.fileids()))\n",
    "english_stop_words = set(stopwords.words('english'))\n",
    "french_stop_words = set(stopwords.words(\"french\"))\n",
    "print(len(english_stop_words))\n",
    "print(len(french_stop_words))\n",
    "\n",
    "stopwords_in_24_languages=set()\n",
    "for language in stopwords.fileids():\n",
    "    stopwords_in_24_languages |= set(stopwords.words(language))\n",
    "print(len(stopwords_in_24_languages))\n",
    "\n",
    "\n",
    "\n",
    "def remove_stopwords_denis(abstract:str,stopwords):\n",
    "    return \",\".join(word for word in abstract.split(\",\") if word not in stopwords)\n",
    "\n",
    "def remove_stopwords_nico(abstract:str,stopwords):\n",
    "    for stop_word in stopwords:\n",
    "        clean=abstract.replace(stop_word+\",\",\"\")\n",
    "    return clean \n",
    "\n",
    "text=\"in,this,paper,we,describe,a,new,bitmap,indexing,technique,to,cluster,xml,xml,is,a,new,standard,for,exchanging,and,representing,information,on,the,documents,can,be,hierarchically,represented,by,xml,documents,are,represented,and,indexed,using,a,bitmap,indexing,we,define,the,similarity,and,popularity,operations,available,in,bitmap,indexes,and,propose,a,method,for,partitioning,a,xml,document,furthermore,a,bitmap,index,is,extended,to,a,bitmap,index,called,we,define,statistical,measurements,in,the,mean,mode,standard,derivation,and,correlation,based,on,these,measurements,we,also,define,the,slice,project,and,dice,operations,on,a,bitcube,can,be,manipulated,efficiently,and,improves,the,performance,of,document,\"\n",
    "start = time.time()\n",
    "remove_stopwords_denis(text,stopwords_in_24_languages)\n",
    "end = time.time()\n",
    "t1=end-start\n",
    "print(\"The time of execution denis :\", t1)\n",
    "start = time.time()\n",
    "remove_stopwords_nico(text,stopwords_in_24_languages)\n",
    "end = time.time()\n",
    "t2=end-start\n",
    "print(\"The time of execution nico :\",t2)\n",
    "print(\"nico gana\",t1>t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40c545317e1f4f027b59ac516c657dca7c8135b6591d53de9168819d5aa6694c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('Deeplearning': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
