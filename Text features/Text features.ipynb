{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Author embedding from abstract embedding\n",
    "We'll consider the author representation to be the average of its paper's representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T11:32:46.075668Z",
     "start_time": "2021-12-08T11:32:32.809676Z"
    }
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T11:39:16.855957Z",
     "start_time": "2021-12-08T11:32:54.489193Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the paper embeddings\n",
    "dict_paper_embeddings = {}\n",
    "id_author = 0\n",
    "vect_str = \"\"\n",
    "\n",
    "\n",
    "with open(\"../Data/paper_embeddings_64.txt\",\"r\") as f_paper:\n",
    "    \n",
    "    for line in f_paper:\n",
    "        if(':' in line):\n",
    "            id_author, vect = line.split(':',1)\n",
    "            vect_str = vect_str + vect.replace('\\n','')\n",
    "        else:\n",
    "            vect_str += line.replace('\\n','')\n",
    "        \n",
    "        if(']' in line):\n",
    "            dict_paper_embeddings[id_author] = ast.literal_eval(vect_str.replace(' ',','))\n",
    "            vect_str = \"\"\n",
    "\n",
    "            \n",
    "# load the list of authors in the training set and the h-index associated\n",
    "f_train = open(\"../Data/train.csv\",\"r\")\n",
    "list_lines = f_train.readlines()\n",
    "dict_h_index = {}\n",
    "\n",
    "for line in list_lines[1:]:\n",
    "    line = line.replace(\"\\n\",\"\").split(\",\",1)\n",
    "    dict_h_index[int(line[0])] = float(line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T04:36:29.845891Z",
     "start_time": "2021-12-08T04:36:28.848930Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(key, value)=(3603,[1.95502567, 0.88846236, -2.28757286, -2.7875936, -1.93649995, -4.76704264, 4.0885787, -1.5846678, 2.7917676, -1.71815956, -1.72307014, -1.24436772, -2.01424241, 1.24009359, 0.72393638, 1.12063658, 2.70689917, 0.73714691, 1.57630599, -0.08534257, -2.05471015, 3.23041511, 0.47726882, 1.1646086, -2.74360585, 1.02150488, 0.41885796, 1.93923664, -0.77844548, -0.70080948, 1.13125944, -0.91544026, 0.34437126, 0.28214422, -0.99386036, 2.38796687, 1.85526562, 1.32367218, 1.39867067, 0.08051338, 0.68632078, -0.87475097, 2.62502384, -2.13532448, 0.91333014, 2.98717117, -1.29266083, 1.3762778, 0.30916947, 1.07678699, -1.50235331, -2.31246638, 1.69867396, -1.21003318, 0.24496719, -1.94066477, -1.09462011, -2.24982524, -0.98247027, -3.76954675, -0.61786431, -1.90634644, 3.11926484, -1.81494617])\n",
      "(key, value)=(7383,[0.12814756, 0.63164562, -0.98055059, 2.10271621, 0.1266593, 0.53838509, 4.39952612, -5.95139265, -0.15614305, -0.5671345, -0.22147386, -0.07161764, 0.13969365, -0.82169539, -4.72962952, 1.9492768, -0.78670841, 0.50053394, -2.07729197, -2.85605025, -0.52120614, 0.50715619, -1.23242402, -1.77672756, 3.43173623, 3.07052493, 1.07087934, -3.13725209, 0.64002681, -0.80843002, 1.24820125, -0.0083334, 2.39691162, 3.96330976, 0.41429037, 3.1915741, 0.67587286, -0.37825024, -2.79865479, 0.8263486, 2.47800875, -2.47860146, 4.20049763, 0.35907239, -0.87613177, 2.62395906, -0.5542537, 0.08426304, 0.08444323, -0.33472171, 0.01973174, -0.31293228, -1.57176471, 1.81387019, -1.91917288, 1.86051714, -1.14755118, 1.48255539, -3.30819011, 3.43145251, -0.5106135, 2.32786918, 1.00376189, -1.93963599])\n",
      "(key, value)=(14309,[-1.09844601, -0.37102315, -2.57387805, -1.15511024, -1.49122632, -1.26658034, -3.03929567, -2.66019964, 0.02797656, -1.72753453, 0.13543856, 2.61542869, -1.76969934, -3.37867951, -3.6787045, 0.61241525, 1.18025124, 3.71606207, 0.77909154, -2.41363978, -0.91544777, -0.7596001, -1.65076578, -1.90210688, -0.70633006, -3.30108309, 1.6578207, -0.4505178, 1.9251554, 2.96485376, 0.54478973, 3.87570477, 1.16021109, 3.26042914, -0.36732447, -1.07333374, 1.69953561, 2.98863578, -1.27668417, -1.7833209, 0.66848272, -0.2807821, 1.57947135, -1.76550007, -0.33592132, 0.68817109, -1.05989945, -0.63831139, -0.56817853, -0.28317744, 0.05441573, -2.65603876, -0.29855129, -1.32980251, 0.18459547, -0.64281553, -2.16382313, 0.36073235, -0.98913348, -0.63927215, -0.85496014, -0.1775512, -0.0988149, 1.38328207])\n"
     ]
    }
   ],
   "source": [
    "# print some exemples of paper embeddings\n",
    "count = 0\n",
    "for key,value in dict_paper_embeddings.items():\n",
    "    if(count > 2):\n",
    "        break\n",
    "    print('(key, value)=({0},{1})'.format(key,value))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T04:37:46.246459Z",
     "start_time": "2021-12-08T04:36:29.987620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of abstracts:  624181\n",
      "number of preprocessed abstracts:  624181\n",
      "number of paper embeddings:  624181\n"
     ]
    }
   ],
   "source": [
    "f_ = open('../Data/abstracts.txt','r',encoding=\"utf8\") \n",
    "list_lines_ = f_.readlines()\n",
    "print(\"number of abstracts: \",len(list_lines_))\n",
    "f_.close()\n",
    "\n",
    "f_ = open('../Data/abstracts_processed.txt','r',encoding=\"utf8\") \n",
    "list_lines_ = f_.readlines()\n",
    "print(\"number of preprocessed abstracts: \",len(list_lines_))\n",
    "f_.close()\n",
    "\n",
    "print('number of paper embeddings: ',len(dict_paper_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the *graph_features.ipynb* file, we have seen that there are however 683606 papers in the *author_papers.txt*. So there are papers without abstracts and potentially authors without any abstract. In this case, we'll consider the *null vector* as the embedding of that author. This decision is based on the fact that a null vector would not affect the calculations in our general models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T04:37:49.793910Z",
     "start_time": "2021-12-08T04:37:47.382384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036332:1510273386-1827736641-1588673897-2252711322-2123653597\n",
      "\n",
      "1101850:133459021-179719743-2111787673-2126488676-31838995\n",
      "\n",
      "1336878:2122092249-2132109814-2100271871-2065672539-2036413831\n",
      "\n",
      "1515524:2141827797-2127085795-2013547785-2138529788-1994863898\n",
      "\n",
      "1606427:1907724546\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"../Data/author_papers.txt\") as f_author_papers:\n",
    "    for i in range(5):\n",
    "        print(f_author_papers.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we'll create the list of embeddings and save it later as .csv file using a pandas Data Frame. We also  calculate the average h-index of the paper authors. This measure will be used to analize a possible clustering based on the papers embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T11:07:46.339408Z",
     "start_time": "2021-12-08T11:05:16.758Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list_authors_id = []\n",
    "list_embeddings = []\n",
    "dict_author_embeddings = {}\n",
    "dict_paper_hindex = {}\n",
    "dict_list_paper_hindex = {}\n",
    "list_aux_emb = []\n",
    "list_aux_hindex = []\n",
    "\n",
    "list_paper_not_embedding = []\n",
    "list_authors_null_embeddig = []\n",
    "\n",
    "dim_author_text_embedding = 64\n",
    "\n",
    "with open(\"../Data/author_papers.txt\") as f_author_papers:\n",
    "    for line in f_author_papers:\n",
    "        author, papers = line.split(':',1)\n",
    "        papers = papers.replace('\\n','').split('-')\n",
    "        papers = list(map(int,papers))\n",
    "\n",
    "        list_authors_id.append(author)\n",
    "        for paper in papers:\n",
    "            if(str(paper) in dict_paper_embeddings):\n",
    "                list_aux_emb.append(dict_paper_embeddings[str(paper)])\n",
    "                dict_list_paper_hindex[paper].append(dict_h_index[int(author)])\n",
    "            else:\n",
    "                list_paper_not_embedding.append(paper)\n",
    "        \n",
    "        author_emb = np.zeros(dim_author_text_embedding)\n",
    "        aver_paper_hindex = 0\n",
    "        \n",
    "        if(len(list_aux) > 0):\n",
    "            author_emb = np.array(list_aux_emb).mean(axis=0)\n",
    "            aver_paper_hindex = np.array(list_aux_hindex).mean(axis=0)\n",
    "        else:\n",
    "            list_authors_null_embeddig.append(author)\n",
    "        \n",
    "        list_embeddings.append(author_emb)\n",
    "        dict_author_embeddings[author] = author_emb\n",
    "        dict_average_hindex[author] = aver_paper_hindex\n",
    "        \n",
    "        list_aux_emb = []\n",
    "        list_aux_hindex = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T04:38:44.012293Z",
     "start_time": "2021-12-08T04:38:42.353827Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two first author's id:\n",
      " ['1036332', '1101850']\n",
      "\n",
      "Embeddings of these two authors:\n",
      " [array([-0.34260672, -0.96811582,  0.54728579, -0.07709934, -1.39273606,\n",
      "        0.81066697,  1.9965124 , -0.76954713,  0.53001419, -2.3792789 ,\n",
      "       -0.38602469, -1.85247579, -1.27895225, -1.48566678,  0.49702405,\n",
      "        0.54610739, -0.36804133, -1.88826811, -2.1111477 , -1.22022029,\n",
      "       -0.56621911,  0.44606569, -0.45144585, -1.2231652 ,  1.52057779,\n",
      "       -0.64532068, -1.88196129,  1.71291772,  0.99850131,  1.88037358,\n",
      "       -0.01732703, -1.57421604, -0.15491208,  0.2035263 ,  0.11807496,\n",
      "        1.45447964, -0.86877872, -0.3217984 , -0.16963881,  1.03490481,\n",
      "       -0.99414181,  1.98070169,  0.6859818 ,  0.48787495,  0.37763525,\n",
      "        1.30986829,  0.91033524, -1.5677906 , -1.01534811, -2.30998271,\n",
      "       -0.7721648 , -0.51481947, -1.10202755,  0.58657267,  0.93586812,\n",
      "        1.73760075, -0.31252533,  1.25387316, -1.0862413 , -0.28046444,\n",
      "        0.90778377,  1.34419771, -1.15006275, -1.29135148]), array([ 0.40738978, -0.31679747, -1.19494336,  0.51145672, -1.83102255,\n",
      "        1.07172417,  2.12015484, -3.05482648, -0.50068172, -0.95782938,\n",
      "        1.09492134,  0.18994926, -0.74706141,  0.8396319 ,  0.69889473,\n",
      "        0.9229436 , -0.53419769, -1.28718783, -0.38830026, -3.04734447,\n",
      "       -1.84200314,  0.23046297, -0.15983039, -0.51317609,  0.34850797,\n",
      "        0.3644848 ,  1.13929144, -1.43716141, -0.22918077,  2.3793046 ,\n",
      "       -0.39107651,  0.58809956,  2.00337937,  2.63827964, -1.34648745,\n",
      "       -0.36657272, -0.9490336 , -0.01599982,  1.04236831,  0.50209318,\n",
      "       -0.47860768, -0.41605577,  0.48673828,  0.36535788, -0.45833078,\n",
      "        0.13867158, -0.90587773,  2.35768365,  0.14069946, -0.6177808 ,\n",
      "        0.11547115, -0.7617674 ,  0.81972264,  0.95770773, -0.30904719,\n",
      "        2.72693774, -0.94105439, -1.27171273, -1.9799273 ,  0.32807892,\n",
      "        0.50225588,  0.33925461,  0.72618744, -0.07722532])]\n",
      "\n",
      "First two papers without embedding (without an abstract):\n",
      " [2724553065, 2621319086]\n"
     ]
    }
   ],
   "source": [
    "print(\"Two first author's id:\\n\",list_authors_id[:2])\n",
    "print(\"\\nEmbeddings of these two authors:\\n\", list_embeddings[:2])\n",
    "print(\"\\nFirst two papers without embedding (without an abstract):\\n\",list_paper_not_embedding[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T04:38:44.181318Z",
     "start_time": "2021-12-08T04:38:44.166379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of list_authors_id:  217801\n",
      "size of list_embeddings:  217801\n",
      "size of list_paper_not_embedding 75277\n",
      "size of list_authors_null_embeddig 1880\n"
     ]
    }
   ],
   "source": [
    "print(\"size of list_authors_id: \",len(list_authors_id))\n",
    "print(\"size of list_embeddings: \", len(list_embeddings))\n",
    "print(\"size of list_paper_not_embedding\", len(list_paper_not_embedding))\n",
    "print(\"size of list_authors_null_embeddig\", len(list_authors_null_embeddig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T04:39:47.108663Z",
     "start_time": "2021-12-08T04:38:44.182282Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "with open(\"../Data/author_embeddings_64.txt\",\"w\", encoding=\"utf8\") as f_author_embeddings_text:\n",
    "    for i in range(len(list_authors_id)):\n",
    "        f_author_embeddings_text.write(str(list_authors_id[i])+\":\"+str(list_embeddings[i])+\"\\n\")\n",
    "'''\n",
    "dim_author_text_embeddings = 64\n",
    "cols_at_emb = [\"at_embedding_\"+str(i) for i in range(dim_author_text_embeddings)]\n",
    "\n",
    "df_author_embeddings = pd.DataFrame()\n",
    "df_author_embeddings['author_id'] = list_authors_id\n",
    "df_author_embeddings[cols_at_emb] = np.array(list_embeddings)\n",
    "df_author_embeddings.to_csv(\"author_embeddings_64.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T04:39:51.417765Z",
     "start_time": "2021-12-08T04:39:47.496136Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>at_embedding_0</th>\n",
       "      <th>at_embedding_1</th>\n",
       "      <th>at_embedding_2</th>\n",
       "      <th>at_embedding_3</th>\n",
       "      <th>at_embedding_4</th>\n",
       "      <th>at_embedding_5</th>\n",
       "      <th>at_embedding_6</th>\n",
       "      <th>at_embedding_7</th>\n",
       "      <th>at_embedding_8</th>\n",
       "      <th>...</th>\n",
       "      <th>at_embedding_54</th>\n",
       "      <th>at_embedding_55</th>\n",
       "      <th>at_embedding_56</th>\n",
       "      <th>at_embedding_57</th>\n",
       "      <th>at_embedding_58</th>\n",
       "      <th>at_embedding_59</th>\n",
       "      <th>at_embedding_60</th>\n",
       "      <th>at_embedding_61</th>\n",
       "      <th>at_embedding_62</th>\n",
       "      <th>at_embedding_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1036332</td>\n",
       "      <td>-0.342607</td>\n",
       "      <td>-0.968116</td>\n",
       "      <td>0.547286</td>\n",
       "      <td>-0.077099</td>\n",
       "      <td>-1.392736</td>\n",
       "      <td>0.810667</td>\n",
       "      <td>1.996512</td>\n",
       "      <td>-0.769547</td>\n",
       "      <td>0.530014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.935868</td>\n",
       "      <td>1.737601</td>\n",
       "      <td>-0.312525</td>\n",
       "      <td>1.253873</td>\n",
       "      <td>-1.086241</td>\n",
       "      <td>-0.280464</td>\n",
       "      <td>0.907784</td>\n",
       "      <td>1.344198</td>\n",
       "      <td>-1.150063</td>\n",
       "      <td>-1.291351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1101850</td>\n",
       "      <td>0.407390</td>\n",
       "      <td>-0.316797</td>\n",
       "      <td>-1.194943</td>\n",
       "      <td>0.511457</td>\n",
       "      <td>-1.831023</td>\n",
       "      <td>1.071724</td>\n",
       "      <td>2.120155</td>\n",
       "      <td>-3.054826</td>\n",
       "      <td>-0.500682</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.309047</td>\n",
       "      <td>2.726938</td>\n",
       "      <td>-0.941054</td>\n",
       "      <td>-1.271713</td>\n",
       "      <td>-1.979927</td>\n",
       "      <td>0.328079</td>\n",
       "      <td>0.502256</td>\n",
       "      <td>0.339255</td>\n",
       "      <td>0.726187</td>\n",
       "      <td>-0.077225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  author_id  at_embedding_0  at_embedding_1  at_embedding_2  at_embedding_3  \\\n",
       "0   1036332       -0.342607       -0.968116        0.547286       -0.077099   \n",
       "1   1101850        0.407390       -0.316797       -1.194943        0.511457   \n",
       "\n",
       "   at_embedding_4  at_embedding_5  at_embedding_6  at_embedding_7  \\\n",
       "0       -1.392736        0.810667        1.996512       -0.769547   \n",
       "1       -1.831023        1.071724        2.120155       -3.054826   \n",
       "\n",
       "   at_embedding_8  ...  at_embedding_54  at_embedding_55  at_embedding_56  \\\n",
       "0        0.530014  ...         0.935868         1.737601        -0.312525   \n",
       "1       -0.500682  ...        -0.309047         2.726938        -0.941054   \n",
       "\n",
       "   at_embedding_57  at_embedding_58  at_embedding_59  at_embedding_60  \\\n",
       "0         1.253873        -1.086241        -0.280464         0.907784   \n",
       "1        -1.271713        -1.979927         0.328079         0.502256   \n",
       "\n",
       "   at_embedding_61  at_embedding_62  at_embedding_63  \n",
       "0         1.344198        -1.150063        -1.291351  \n",
       "1         0.339255         0.726187        -0.077225  \n",
       "\n",
       "[2 rows x 65 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_author_embeddings[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Bags Of Words (BOW) for the most frequent words\n",
    "The representation of an abstract as a vector couting the number of occurrences of each word in all the abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "def abstract_count_words():\n",
    "    '''\n",
    "    Count the number of non_stop words in each of the abstracts\n",
    "    '''\n",
    "    file =open(\"../Data/abstracts_processed.txt\",\"r\",encoding=\"utf8\")\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    abstract_text=[]\n",
    "    for abstract in file:      \n",
    "        if(abstract==\"\\n\"):\n",
    "            continue\n",
    "        paper_id,abstract = abstract.split(\"----\",1)\n",
    "        abstract_text.append(abstract.replace(\",\",\" \"))\n",
    "    file.close()        \n",
    "    vectorizer = CountVectorizer(stop_words=stop_words, max_features=100)\n",
    " \n",
    "    X = vectorizer.fit_transform(abstract_text)\n",
    "    count_vect_df = pd.DataFrame(X.todense(), columns=vectorizer.get_feature_names())\n",
    "    #print(count_vect_df)\n",
    "    count_vect_df.to_json(\"../Data/bag_of_words.json\",orient=\"records\")\n",
    "    \n",
    "abstract_count_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_bow = open(\"../Data/bag_of_words.json\",\"r\",encoding=\"utf8\")\n",
    "data = json.load(file_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. BOW for all the words\n",
    "\n",
    "The execution of the Bag Of Words (BOW) algorithm for all the abstracts is too computational expensive. We tried but it never finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3603----in,this,paper,we,describe,a,new,bitmap,indexing,technique,to,cluster,xml,xml,is,a,new,standard,for,exchanging,and,representing,information,on,the,documents,can,be,hierarchically,represented,by,xml,documents,are,represented,and,indexed,using,a,bitmap,indexing,we,define,the,similarity,and,popularity,operations,available,in,bitmap,indexes,and,propose,a,method,for,partitioning,a,xml,document,furthermore,a,bitmap,index,is,extended,to,a,bitmap,index,called,we,define,statistical,measurements,in,the,mean,mode,standard,derivation,and,correlation,based,on,these,measurements,we,also,define,the,slice,project,and,dice,operations,on,a,bitcube,can,be,manipulated,efficiently,and,improves,the,performance,of,document,\n",
      "\n",
      "7383----the,paper,starts,from,the,observation,that,in,the,approach,to,geometry,there,are,serious,difficulties,in,defining,these,difficulties,disappear,once,we,reformulate,this,approach,in,the,framework,of,continuous,multivalued,so,a,theory,of,is,proposed,as,a,counterpart,of,the,usual,of,again,a,second,theory,is,considered,in,which,the,graded,predicates,be,and,be,are,assumed,as,in,both,cases,a,suitable,notion,of,abstractive,sequence,and,of,equivalence,between,abstractive,sequences,enables,us,to,define,the,in,the,resulting,set,of,points,a,distance,is,defined,in,a,natural,way,and,this,enables,a,metrical,approach,to,geometry,and,therefore,to,go,beyond,general,idea,is,that,it,is,possible,to,search,for,mathematical,formalizations,of,the,naive,theory,of,the,space,an,ordinary,man,needs,to,have,in,its,everyday,to,do,this,we,have,to,direct,our,attention,not,only,to,regions,and,the,related,relation,of,inclusion,as,it,is,usual,in,geometry,but,also,to,those,properties,which,are,geometrical,in,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open(\"../Data/abstracts_processed.txt\")\n",
    "for i in range(2):\n",
    "    print(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['paper', 'describe', 'new', 'bitmap', 'indexing', 'technique', 'cluster', 'xml', 'standard', 'exchanging', 'representing', 'information', 'documents', 'hierarchically', 'represented', 'indexed', 'using', 'define', 'similarity', 'popularity', 'operations', 'available', 'indexes', 'propose', 'method', 'partitioning', 'document', 'furthermore', 'index', 'extended', 'called', 'statistical', 'measurements', 'mean', 'mode', 'derivation', 'correlation', 'based', 'also', 'slice', 'project', 'dice', 'bitcube', 'manipulated', 'efficiently', 'improves', 'performance', '\\n', 'starts', 'observation', 'approach', 'geometry', 'serious', 'difficulties', 'defining', 'disappear', 'reformulate', 'framework', 'continuous', 'multivalued', 'theory', 'proposed', 'counterpart', 'usual', 'second', 'considered', 'graded', 'predicates', 'assumed', 'cases', 'suitable', 'notion', 'abstractive', 'sequence', 'equivalence', 'sequences', 'enables', 'us', 'resulting', 'set', 'points', 'distance', 'defined', 'natural', 'way', 'metrical', 'therefore', 'go', 'beyond', 'general', 'idea', 'possible', 'search', 'mathematical', 'formalizations', 'naive', 'space', 'ordinary', 'man', 'needs', 'everyday', 'direct', 'attention', 'regions', 'related', 'relation', 'inclusion', 'properties', 'geometrical']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def abstract_distinct_words_count(filename=\"../Data/abstracts_processed.txt\"):\n",
    "    '''\n",
    "    Count the number of non_stop words in each of the abstracts\n",
    "    '''\n",
    "    file =open(filename,\"r\")\n",
    "#     fw = open(\"all_abstracts.txt\",\"w\")\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    \n",
    "     \n",
    "  \n",
    "    unique_abstract_words=[]\n",
    "    for abstract in file:       \n",
    "        if(abstract==\"\\n\"):\n",
    "            continue\n",
    "        paper_id,abstract = abstract.split(\"----\",1)\n",
    "        words_in_abstract=abstract.split(\",\")\n",
    "        for word in words_in_abstract:\n",
    "            if word not in stop_words:\n",
    "                if word not in unique_abstract_words:\n",
    "                    unique_abstract_words.append(word)\n",
    "       \n",
    "    file.close()  \n",
    "    print(unique_abstract_words)\n",
    "    return len(unique_abstract_words)\n",
    "\n",
    "abstract_distinct_words_count(filename=\"../Data/abstracts_processed_test.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
