{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T23:37:25.210649Z",
     "start_time": "2021-12-07T23:37:25.116379Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error,accuracy_score\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data loading and data frame creation\n",
    "\n",
    "The code in this section must be executed together, even if it is divided into different cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T23:37:28.120312Z",
     "start_time": "2021-12-07T23:37:27.110847Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_train: 174241, n_test: 43560\n"
     ]
    }
   ],
   "source": [
    "# read author id and h_index for training\n",
    "df_train = pd.read_csv('../Data/train.csv', dtype={'author': np.int64,'h_index': np.float32})\n",
    "df_test = pd.read_csv('../Data/test.csv', index_col=0, dtype={'author': np.int64})\n",
    "n_train = df_train.shape[0]\n",
    "n_test = df_test.shape[0]\n",
    "print(\"n_train: {0}, n_test: {1}\".format(n_train,n_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T23:37:31.737247Z",
     "start_time": "2021-12-07T23:37:28.122306Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data.\n"
     ]
    }
   ],
   "source": [
    "# read author embedding from graph nodes\n",
    "dim_embeddings = 32\n",
    "cols_embeddings = [\"n_embedding_\"+str(i) for i in range(dim_embeddings)]\n",
    "dict_dtype = {'author_id':np.int64}\n",
    "for i in range(dim_embeddings):\n",
    "    dict_dtype['n_embeddings_'+str(i)] = np.float32\n",
    "df_node_emb=pd.read_csv(\"../Data/node_embeddings_20.csv\",sep=',',dtype=dict_dtype)\n",
    "\n",
    "# read author embedding from abstracts\n",
    "dim_embeddings = 64\n",
    "cols_embeddings = [\"at_embedding_\"+str(i) for i in range(dim_embeddings)]\n",
    "dict_dtype = {'author_id':np.int64}\n",
    "for i in range(dim_embeddings):\n",
    "    dict_dtype['at_embeddings_'+str(i)] = np.float32\n",
    "df_author_emb=pd.read_csv(\"../Data/author_embeddings_64.csv\",dtype=dict_dtype)\n",
    "'''\n",
    "# version 1 of graph features\n",
    "dict_dtype = {'author_id':np.int64,'core_number':np.float32,'clustering_coef':np.float32,\n",
    "             'betweeness_coef':np.float32,'centrality':np.float32,'page_rank':np.float32,\n",
    "              'clustering_coef_coauthorship':np.float32,'betweeness_coef_coauthorship':np.float32,\n",
    "              'centrality_coauthorship':np.float32,'page_rank_coauthorship':np.float32}\n",
    "df_graph_feat=pd.read_csv(\"../Data/graph_features.csv\", dtype=dict_dtype)\n",
    "'''\n",
    "# version 2 of graph features\n",
    "dict_dtype = {'author_id':np.int64,'core_number':np.float32,'clustering_coef':np.float32,\n",
    "             'betweeness_coef':np.float32,'centrality':np.float32,'page_rank':np.float32,\n",
    "              'clustering_coef_coauthorship':np.float32,'betweeness_coef_coauthorship':np.float32,\n",
    "              'centrality_coauthorship':np.float32,'page_rank_coauthorship':np.float32,\n",
    "              'degree':np.int32,'weighted_degree':np.float32,'onion_number':np.float32\n",
    "              }\n",
    "df_graph_feat=pd.read_csv(\"../Data/graph_features_v2.csv\", dtype=dict_dtype)\n",
    "\n",
    "print(\"Loaded data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T23:37:33.166886Z",
     "start_time": "2021-12-07T23:37:31.739218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names X_train: \n",
      " ['n_embedding_0', 'n_embedding_1', 'n_embedding_2', 'n_embedding_3', 'n_embedding_4', 'n_embedding_5', 'n_embedding_6', 'n_embedding_7', 'n_embedding_8', 'n_embedding_9', 'n_embedding_10', 'n_embedding_11', 'n_embedding_12', 'n_embedding_13', 'n_embedding_14', 'n_embedding_15', 'n_embedding_16', 'n_embedding_17', 'n_embedding_18', 'n_embedding_19', 'at_embedding_0', 'at_embedding_1', 'at_embedding_2', 'at_embedding_3', 'at_embedding_4', 'at_embedding_5', 'at_embedding_6', 'at_embedding_7', 'at_embedding_8', 'at_embedding_9', 'at_embedding_10', 'at_embedding_11', 'at_embedding_12', 'at_embedding_13', 'at_embedding_14', 'at_embedding_15', 'at_embedding_16', 'at_embedding_17', 'at_embedding_18', 'at_embedding_19', 'at_embedding_20', 'at_embedding_21', 'at_embedding_22', 'at_embedding_23', 'at_embedding_24', 'at_embedding_25', 'at_embedding_26', 'at_embedding_27', 'at_embedding_28', 'at_embedding_29', 'at_embedding_30', 'at_embedding_31', 'at_embedding_32', 'at_embedding_33', 'at_embedding_34', 'at_embedding_35', 'at_embedding_36', 'at_embedding_37', 'at_embedding_38', 'at_embedding_39', 'at_embedding_40', 'at_embedding_41', 'at_embedding_42', 'at_embedding_43', 'at_embedding_44', 'at_embedding_45', 'at_embedding_46', 'at_embedding_47', 'at_embedding_48', 'at_embedding_49', 'at_embedding_50', 'at_embedding_51', 'at_embedding_52', 'at_embedding_53', 'at_embedding_54', 'at_embedding_55', 'at_embedding_56', 'at_embedding_57', 'at_embedding_58', 'at_embedding_59', 'at_embedding_60', 'at_embedding_61', 'at_embedding_62', 'at_embedding_63', 'core_number', 'clustering_coef', 'betweeness_coef', 'centrality', 'page_rank', 'degree', 'onion_number', 'weighted_degree']\n",
      "Created X,y for training.\n",
      "Dimensions of X_train:  (174241, 92)\n",
      "Dimensions of y_train:  (174241,)\n",
      "\n",
      "Column names X_test: \n",
      " ['n_embedding_0', 'n_embedding_1', 'n_embedding_2', 'n_embedding_3', 'n_embedding_4', 'n_embedding_5', 'n_embedding_6', 'n_embedding_7', 'n_embedding_8', 'n_embedding_9', 'n_embedding_10', 'n_embedding_11', 'n_embedding_12', 'n_embedding_13', 'n_embedding_14', 'n_embedding_15', 'n_embedding_16', 'n_embedding_17', 'n_embedding_18', 'n_embedding_19', 'at_embedding_0', 'at_embedding_1', 'at_embedding_2', 'at_embedding_3', 'at_embedding_4', 'at_embedding_5', 'at_embedding_6', 'at_embedding_7', 'at_embedding_8', 'at_embedding_9', 'at_embedding_10', 'at_embedding_11', 'at_embedding_12', 'at_embedding_13', 'at_embedding_14', 'at_embedding_15', 'at_embedding_16', 'at_embedding_17', 'at_embedding_18', 'at_embedding_19', 'at_embedding_20', 'at_embedding_21', 'at_embedding_22', 'at_embedding_23', 'at_embedding_24', 'at_embedding_25', 'at_embedding_26', 'at_embedding_27', 'at_embedding_28', 'at_embedding_29', 'at_embedding_30', 'at_embedding_31', 'at_embedding_32', 'at_embedding_33', 'at_embedding_34', 'at_embedding_35', 'at_embedding_36', 'at_embedding_37', 'at_embedding_38', 'at_embedding_39', 'at_embedding_40', 'at_embedding_41', 'at_embedding_42', 'at_embedding_43', 'at_embedding_44', 'at_embedding_45', 'at_embedding_46', 'at_embedding_47', 'at_embedding_48', 'at_embedding_49', 'at_embedding_50', 'at_embedding_51', 'at_embedding_52', 'at_embedding_53', 'at_embedding_54', 'at_embedding_55', 'at_embedding_56', 'at_embedding_57', 'at_embedding_58', 'at_embedding_59', 'at_embedding_60', 'at_embedding_61', 'at_embedding_62', 'at_embedding_63', 'core_number', 'clustering_coef', 'betweeness_coef', 'centrality', 'page_rank', 'degree', 'onion_number', 'weighted_degree']\n",
      "Created X,y for testing.\n",
      "Dimensions of X_test:  (43560, 92)\n",
      "Dimensions of y_test:  (43560,)\n"
     ]
    }
   ],
   "source": [
    "# create the training dataframe.\n",
    "df_train = df_train.merge(df_node_emb, left_on=\"author\", right_on=\"author_id\")\n",
    "df_train = df_train.merge(df_author_emb, left_on=\"author\", right_on=\"author_id\")\n",
    "df_train = df_train.merge(df_graph_feat, left_on=\"author\", right_on=\"author_id\")\n",
    "\n",
    "X_id_train = df_train[\"author\"].values\n",
    "y_train = df_train[\"hindex\"].values\n",
    "\n",
    "\n",
    "features_to_drop = [\"author\",\"hindex\",\"author_id_x\",\"author_id_y\",\n",
    "                    \"clustering_coef_coauthorship\",\"betweeness_coef_coauthorship\",\n",
    "                    \"centrality_coauthorship\",\"page_rank_coauthorship\",\"author_id\"]\n",
    "'''\n",
    "features_to_drop = [\"author\",\"hindex\",\"author_id_x\",\"author_id_y\",\n",
    "                    \"betweeness_coef_coauthorship\",\n",
    "                    \"author_id\"]\n",
    "'''\n",
    "\n",
    "df_train = df_train.drop(features_to_drop,axis=1)\n",
    "X_train = df_train.values\n",
    "print(\"Column names X_train: \\n\",df_train.columns.tolist())\n",
    "print(\"Created X,y for training.\")\n",
    "print(\"Dimensions of X_train: \",X_train.shape)\n",
    "print(\"Dimensions of y_train: \",y_train.shape)\n",
    "\n",
    "# create the test dataframe. \n",
    "df_test = df_test.merge(df_node_emb, left_on=\"author\", right_on=\"author_id\")\n",
    "df_test = df_test.merge(df_author_emb, left_on=\"author\", right_on=\"author_id\")\n",
    "df_test = df_test.merge(df_graph_feat, left_on=\"author\", right_on=\"author_id\")\n",
    "\n",
    "X_id_test = df_test[\"author\"].values\n",
    "y_test = df_test[\"hindex\"].values\n",
    "\n",
    "df_test = df_test.drop(features_to_drop,axis=1)\n",
    "X_test = df_test.values\n",
    "print(\"\\nColumn names X_test: \\n\",df_test.columns.tolist())\n",
    "print(\"Created X,y for testing.\")\n",
    "print(\"Dimensions of X_test: \",X_test.shape)\n",
    "print(\"Dimensions of y_test: \",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T00:38:09.498551Z",
     "start_time": "2021-12-08T00:38:09.484588Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## lightgbm\n",
    "\n",
    "def lightgbm(X_train, y_train,metrics:list=['mean_squared_error'], \n",
    "                  transformation=None,transformation_inv=None, \n",
    "                  n_splits=3,seed=2,n_jobs=1,colsample_bytree=0.4,\n",
    "                  alpha=0.05,learning_rate=0.025,n_estimators=6000):\n",
    "    \n",
    "    '''\n",
    "    X_train:training data with dependent variables. It can be a dataframe or a numpy matrix.\n",
    "    y_train: training data with independent variable (h-index). It can be a dataframe or a numpy matrix.\n",
    "    metrics: metrics to calculate the model performance. It is a list of strings. Only 'mse' and 'accuracy' supported.\n",
    "    transformation: a function to apply to the partitions of y_train.\n",
    "    transformation_inv: the inverse of transformation. It is applied to the predictions of the y_train partitions.\n",
    "    val_size: proportion of the y_train to include y_val.\n",
    "    seed: random number seed.\n",
    "    n_jobs: number of CPUs to use during the cross validation\n",
    "    colsample_bytree: subsample ratio of columns (features) when constructing each tree.\n",
    "    alpha: parameter for Huber loss and Quantile regression.\n",
    "    learning_rate: boosting learning rate.\n",
    "    n_estimators: number of boosted trees to fit\n",
    "    '''\n",
    "    \n",
    "    if(transformation is not None):\n",
    "        try:\n",
    "            y_train = transformation(y_train)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    print(\"Start lightgbm fitting:\\n\")\n",
    "    reg = lgb.LGBMRegressor(is_unbalance=True,\n",
    "                        colsample_bytree=colsample_bytree, \n",
    "                        importance_type='gain', alpha=alpha,\n",
    "                        objective='mse', learning_rate=learning_rate,\n",
    "                        n_estimators=n_estimators,\n",
    "                        n_jobs=n_jobs\n",
    "                        )\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_splits,shuffle=True)\n",
    "    list_mse_train = []\n",
    "    list_mse_val = []\n",
    "    list_accuracy_train = []\n",
    "    list_accuracy_val = []\n",
    "\n",
    "    count = 0\n",
    "    for train_index, test_index in skf.split(X_train, y_train):\n",
    "        count += 1\n",
    "        X_tr, X_val = X_train[train_index], X_train[test_index]\n",
    "        y_tr, y_val = y_train[train_index], y_train[test_index]\n",
    "        reg.fit(X_tr, y_tr)\n",
    "        mse_train = mean_squared_error(y_tr,reg.predict(X_tr))\n",
    "        mse_val = mean_squared_error(y_val,reg.predict(X_val))\n",
    "        \n",
    "        if(transformation_inv is not None):\n",
    "            try:\n",
    "                y_pred_train = transformation_inv(y_pred_train)\n",
    "                y_pred_val = transformation_inv(y_pred_val)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if('mean_squared_error' in metrics):\n",
    "            list_mse_train.append(mse_train)\n",
    "            list_mse_val.append(mse_val)\n",
    "        if('accuracy' in metrics):\n",
    "            list_accuracy_train.append(accuracy_score(y_tr,reg.predict(X_tr)))\n",
    "            list_accuracy_val.append(accuracy_score(y_val,reg.predict(X_val)))\n",
    "        print(\"Round: {0}, mean_squared_error on train: {1}, mean_squared_error on test: {2}\".format(count,mse_train,mse_val));\n",
    "    \n",
    "    \n",
    "    config = \"learning_rate: \"+str(learning_rate)+\", n_splits: \"+str(n_splits)\n",
    "    config += \", colsample_bytree: \"+str(colsample_bytree)+\", alpha: \"\n",
    "    config += str(alpha)+\", n_estimators: \"+str(n_estimators)\n",
    "    print(\"\\nConfig:\",config)\n",
    "    \n",
    "    if('mean_squared_error' in metrics):\n",
    "        print(\"Lightgbm mean squared error on train:\",np.mean(list_mse_train))\n",
    "        print(\"Lightgbm mean squared error on test:\",np.mean(list_mse_val))\n",
    "    if('accuracy' in metrics):\n",
    "        print(\"Lightgbm accuracy on train:\",np.mean(list_accuracy_train))\n",
    "        print(\"Lightgbm accuracy on test with:\",np.mean(list_acuracy_val))\n",
    "    \n",
    "    print(\"Prediction of null features: \",reg.predict(np.zeros((1,len(X_tr[0]))))[0])\n",
    "    \n",
    "    return reg,list_mse_train,list_mse_val,config\n",
    "\n",
    "#best_model = lightgbm(X_train, y_train)\n",
    "#lightgbm(X_train, y_train, transformation=np.log10,transformation_inv=lambda array: np.power(10,array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T00:55:38.498648Z",
     "start_time": "2021-12-08T00:38:35.511616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start lightgbm fitting:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leder\\anaconda3\\envs\\learning2\\lib\\site-packages\\sklearn\\model_selection\\_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=9.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 1, mean_squared_error on train: 4.298185080599955, mean_squared_error on test: 49.52260932079628\n",
      "Round: 2, mean_squared_error on train: 4.332692514483274, mean_squared_error on test: 49.366949688926255\n",
      "Round: 3, mean_squared_error on train: 4.357161506309602, mean_squared_error on test: 47.24373397721559\n",
      "Round: 4, mean_squared_error on train: 4.306208181907501, mean_squared_error on test: 48.71051594550257\n",
      "Round: 5, mean_squared_error on train: 4.319044267470873, mean_squared_error on test: 49.68514837673011\n",
      "Round: 6, mean_squared_error on train: 4.352845557290925, mean_squared_error on test: 50.09833710118123\n",
      "Round: 7, mean_squared_error on train: 4.324301059464739, mean_squared_error on test: 48.92794191938997\n",
      "Round: 8, mean_squared_error on train: 4.43883596863128, mean_squared_error on test: 47.97913364701657\n",
      "Round: 9, mean_squared_error on train: 4.367181769606134, mean_squared_error on test: 48.72138360611254\n",
      "\n",
      "Config: learning_rate: 0.05, n_splits: 9, colsample_bytree: 0.4, alpha: 0.05, n_estimators: 10000\n",
      "Lightgbm mean squared error on train: 4.344050656196032\n",
      "Lightgbm mean squared error on test: 48.91730595365235\n",
      "Prediction of null features:  0.8734598521723994\n"
     ]
    }
   ],
   "source": [
    "learning_rate = [0.05]\n",
    "n_splits = [9]\n",
    "colsample_bytree = [0.4]\n",
    "alpha = [0.05]\n",
    "n_estimators = [10000]\n",
    "\n",
    "#colsample_bytree = [0.2,0.4]\n",
    "#alpha = [0.01,0.05,0.1]\n",
    "#n_estimators=[4000,6000,8000]\n",
    "\n",
    "list_config = []\n",
    "list_mse_tr = []\n",
    "list_mse_val = []\n",
    "\n",
    "min_mse_val = np.inf\n",
    "best_model = None\n",
    "best_config = None\n",
    "\n",
    "for l_r in learning_rate:\n",
    "    for n_s in n_splits:\n",
    "        for csbt in colsample_bytree:\n",
    "            for alp in alpha:\n",
    "                for n_e in n_estimators:   \n",
    "                    model,mse_tr,mse_val,config = lightgbm(X_train,y_train,n_splits=n_s,learning_rate=l_r,\n",
    "                                                   colsample_bytree=csbt,alpha=alp,\n",
    "                                                   n_estimators=n_e,n_jobs=4)\n",
    "                    if(min_mse_val > float(mse_val[len(mse_val)-1])):\n",
    "                        min_mse_val = float(mse_val[len(mse_val)-1])\n",
    "                        best_model = model\n",
    "                        best_config = config\n",
    "                    \n",
    "                    list_config.append(config)\n",
    "                    list_mse_tr.append(mse_tr[len(mse_tr)-1])\n",
    "                    list_mse_val.append(mse_val[len(mse_val)-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T00:29:42.276927Z",
     "start_time": "2021-12-08T00:17:27.103378Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start lightgbm fitting:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leder\\anaconda3\\envs\\learning2\\lib\\site-packages\\sklearn\\model_selection\\_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=7.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 1, mean_squared_error on train: 4.0302409349629436, mean_squared_error on test: 54.12540992740144\n",
      "Round: 2, mean_squared_error on train: 4.126217089698763, mean_squared_error on test: 48.46673601194754\n",
      "Round: 3, mean_squared_error on train: 4.106172070769131, mean_squared_error on test: 47.759054141192145\n",
      "Round: 4, mean_squared_error on train: 4.1085154827925425, mean_squared_error on test: 52.32106331406688\n",
      "Round: 5, mean_squared_error on train: 4.054965685226876, mean_squared_error on test: 50.552161299282886\n",
      "Round: 6, mean_squared_error on train: 4.107422803002861, mean_squared_error on test: 49.087722647223\n",
      "Round: 7, mean_squared_error on train: 4.056721266597967, mean_squared_error on test: 43.72697983372407\n",
      "\n",
      "Config: learning_rate: 0.05, n_splits: 7, colsample_bytree: 0.4, alpha: 0.05, n_estimators: 10000\n",
      "Lightgbm mean squared error on train: 4.084322190435869\n",
      "Lightgbm mean squared error on test: 49.43416102497685\n",
      "Prediction of null features:  0.5902981019968244\n"
     ]
    }
   ],
   "source": [
    "model,list_losses_tr,list_losses_val,_ = lightgbm(X_train,y_train,n_splits=7,learning_rate=0.05,\n",
    "                                                   colsample_bytree=0.4,alpha=0.05,\n",
    "                                                   n_estimators=10000,n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T00:11:00.291189Z",
     "start_time": "2021-12-08T00:10:51.761968Z"
    }
   },
   "outputs": [],
   "source": [
    "# saving predictions for the test data\n",
    "\n",
    "h_test = model.predict(X_test)\n",
    "predictions = pd.DataFrame()\n",
    "predictions['author'] = X_id_test\n",
    "predictions['hindex'] = h_test\n",
    "predictions.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(1, 1, figsize=(16,8))\n",
    "ax1.plot([loss.cpu() for loss in list_losses_tr][:100], c='b', label='train')\n",
    "ax1.plot([loss.cpu() for loss in list_losses_val][:100], c='r', label='test')\n",
    "ax1.set_title(\"Train and validation accuracy\")\n",
    "ax1.set_xlabel(\"Iterations\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
